{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ecb8a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, SpatialDropout1D\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, GRU \n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score \n",
    "import matplotlib.pyplot as plt \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from tensorflow.compat.v1.keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cac6e1",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebbcd23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = tfds.load(\"imdb_reviews\",as_supervised= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a62b1e9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <PrefetchDataset shapes: ((), ()), types: (tf.string, tf.int64)>,\n",
       " 'test': <PrefetchDataset shapes: ((), ()), types: (tf.string, tf.int64)>,\n",
       " 'unsupervised': <PrefetchDataset shapes: ((), ()), types: (tf.string, tf.int64)>}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e63e905",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set= imdb[\"train\"]\n",
    "test_set = imdb[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d6df3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentence = []\n",
    "train_label = []\n",
    "test_sentence = []\n",
    "test_label = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9d56551",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence, label in test_set:\n",
    "    test_sentence.append(str(sentence.numpy()))\n",
    "    test_label.append(label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ef16dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence, label in train_set:\n",
    "    train_sentence.append(str(sentence.numpy()))\n",
    "    train_label.append(label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d7745c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 25000 25000 25000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_sentence), len(train_label), len(test_sentence), len(test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "238c634b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'> <class 'list'> <class 'str'> <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_sentence[0]), type(train_label), type(test_sentence[0]), type(test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8ed59b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array(train_label)\n",
    "test_labels = np.array(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1fdc0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_label), type(test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c44af3b",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "900e9dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "embedding_dim = 16\n",
    "max_length = 234\n",
    "num_epoch =5\n",
    "trunc_type = \"post\"\n",
    "oov_tok = \"<OOV>\"\n",
    "n_lstm = 256\n",
    "drop_lstm = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca980572",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "299fb803",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "625ef105",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(train_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0dc4e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_to_sequences_sentence = tokenizer.texts_to_sequences(train_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53f06b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_to_sequences_sentence = tokenizer.texts_to_sequences(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eabdc3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[59, 12, 14, 35, 439, 400, 18, 174, 29, 1, 9, 33, 1, 1, 42, 496, 1, 197, 25, 88, 156, 19, 12, 211, 340, 29, 70, 248, 213, 9, 486, 62, 70, 88, 116, 99, 24, 1, 12, 1, 657, 777, 12, 18, 7, 35, 406, 1, 178, 1, 426, 2, 92, 1, 140, 72, 149, 55, 2, 1, 1, 72, 229, 70, 1, 16, 1, 1, 1, 1, 1, 1, 3, 40, 1, 119, 1, 17, 1, 14, 163, 19, 4, 1, 927, 1, 9, 4, 18, 13, 14, 1, 5, 102, 148, 1, 11, 240, 692, 13, 44, 25, 101, 39, 12, 1, 1, 39, 1, 1, 52, 409, 11, 99, 1, 874, 145, 10]\n"
     ]
    }
   ],
   "source": [
    "print(train_to_sequences_sentence[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154d9150",
   "metadata": {},
   "source": [
    "# Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5bdca70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padded = pad_sequences(train_to_sequences_sentence, maxlen= max_length, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b45734c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_padded = pad_sequences(test_to_sequences_sentence,maxlen= max_length, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9d3f980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 234) (25000, 234)\n"
     ]
    }
   ],
   "source": [
    "print(train_padded.shape, test_padded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdafca77",
   "metadata": {},
   "source": [
    "# Developing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3ce7f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size,output_dim=embedding_dim, input_length=max_length),\n",
    "    LSTM(units=n_lstm, dropout=drop_lstm),\n",
    "    Flatten(),\n",
    "    Dense(1,activation=\"sigmoid\")\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59a9506b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 234, 16)           16000     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 256)               279552    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 295,809\n",
      "Trainable params: 295,809\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b8ce3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = \"binary_crossentropy\",\n",
    "             optimizer = \"adam\",\n",
    "             metrics = [\"accuracy\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e1a061a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 183s 7ms/sample - loss: 0.6029 - accuracy: 0.6658 - val_loss: 0.4845 - val_accuracy: 0.7810\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 184s 7ms/sample - loss: 0.6110 - accuracy: 0.6667 - val_loss: 0.6121 - val_accuracy: 0.6706\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 186s 7ms/sample - loss: 0.5041 - accuracy: 0.7604 - val_loss: 0.4545 - val_accuracy: 0.7904\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 196s 8ms/sample - loss: 0.4246 - accuracy: 0.8105 - val_loss: 0.4226 - val_accuracy: 0.8063\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 186s 7ms/sample - loss: 0.3664 - accuracy: 0.8416 - val_loss: 0.3525 - val_accuracy: 0.8441\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7facec394a50>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_padded,train_labels,\n",
    "         epochs = num_epoch,\n",
    "         validation_data = (test_padded, test_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a065f137",
   "metadata": {},
   "source": [
    "#### Report on Length of the sequence\n",
    "    - IMDB dataset, all variables fixed, comparing sentences of length 120, vs 234.  Sentences of length 234 achieves higher accuracy on training, and lower loss on training, the same goes for val acc and val loss.  234 is close to the mean length of the sequences. Just from this observation, one can make a statement that selecting a sequence length closer to the mean length of sequences for the dataset, achieves a performance of higher accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505ba0df",
   "metadata": {},
   "source": [
    "# Testing on a sample example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "caf5fadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_1 = [\"This movie is the worst thing ever I watched\",\n",
    "           \"I hate the actor in this movie\",\n",
    "              \"This movie is fucking awful\",\n",
    "              \"This movie is the most terrible movie i have watched with horrible actor\",\n",
    "           \"Do not watch this movie because this movie is really bad\",\n",
    "           \"Fuck this movie. It is really bad\",\n",
    "           \"This movie is the best\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b0fd8721",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_1_text_to_sequence = tokenizer.texts_to_sequences(sentence_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3e19cf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_1_sequence_padded = pad_sequences(sentence_1_text_to_sequence,maxlen = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fcf860ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11041529],\n",
       "       [0.50526756],\n",
       "       [0.13679671],\n",
       "       [0.0671244 ],\n",
       "       [0.24540883],\n",
       "       [0.31215274],\n",
       "       [0.76097846]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_probabilities= model.predict_proba(sentence_1_sequence_padded)\n",
    "output_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "952df0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "This movie is the worst thing ever I watched \n",
      " ['0.11%']\n",
      "\n",
      "\n",
      "I hate the actor in this movie \n",
      " ['0.51%']\n",
      "\n",
      "\n",
      "This movie is fucking awful \n",
      " ['0.14%']\n",
      "\n",
      "\n",
      "This movie is the most terrible movie i have watched with horrible actor \n",
      " ['0.07%']\n",
      "\n",
      "\n",
      "Do not watch this movie because this movie is really bad \n",
      " ['0.25%']\n",
      "\n",
      "\n",
      "Fuck this movie. It is really bad \n",
      " ['0.31%']\n",
      "\n",
      "\n",
      "This movie is the best \n",
      " ['0.76%']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(output_probabilities)):\n",
    "    print(\"\\n\")\n",
    "    print(sentence_1[i], \"\\n\", list(map('{:.2f}%'.format,output_probabilities[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b81cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2e0bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
